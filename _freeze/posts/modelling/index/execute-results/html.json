{
  "hash": "2e4ac4dd4866647aedeb6735fceb1dcf",
  "result": {
    "markdown": "---\ntitle: Modelling\ndescription: Code recipes for modelling in R\nformat: html\neditor: source\ncategories: [r]\ntoc: true\ntoc-depth: 6\ntoc-location: right\nfontsize: medium #small, medium, large, 1.2em, 12px, ...\nexecute:\n  echo: true #whether to include code block\n  eval: true #whether to run code\n  warning: false #include warnings\n  error: false #include errors\n  output: true #include output\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(yingtools2)\nlibrary(broom)\nlibrary(modelr)\nlibrary(tidyverse)\nmt <- mtcars %>% mutate(cyl=factor(cyl))\n```\n:::\n\n\n\n## Linear Regression\n\n### Linear Regression (single var)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- lm(mpg ~ hp,data=mt)\nmt$yhat <- predict(model)\ntidy(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)  30.1       1.63       18.4  6.64e-18\n2 hp           -0.0682    0.0101     -6.74 1.79e- 7\n```\n:::\n\n```{.r .cell-code}\nggplot(mt) +\n  geom_point(aes(x=hp,y=mpg)) +\n  geom_line(aes(x=hp,y=yhat),color=\"red\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n### Linear Regression, show confidence and prediction (single var)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- lm(mpg ~ hp,data=mt)\nci <- predict(model,interval=\"confidence\") %>% cbind(mt)\npi <- predict(model,interval=\"prediction\") %>% cbind(mt)\ntidy(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)  30.1       1.63       18.4  6.64e-18\n2 hp           -0.0682    0.0101     -6.74 1.79e- 7\n```\n:::\n\n```{.r .cell-code}\nggplot(mt) +\n  geom_point(aes(x=hp,y=mpg)) +\n  geom_line(data=pi,aes(x=hp,y=fit,color=\"predicted\")) + \n  geom_ribbon(data=pi,aes(x=hp,ymin=lwr,ymax=upr,fill=\"predicted\"),alpha=0.35,show.legend=FALSE) +\n  geom_line(data=ci,aes(x=hp,y=fit,color=\"confidence\")) + \n  geom_ribbon(data=ci,aes(x=hp,ymin=lwr,ymax=upr,fill=\"confidence\"),alpha=0.35,show.legend=FALSE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n\n\n### Linear Regression (multiple vars)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- lm(mpg ~ hp + cyl, data=mt)\ntidy(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)  28.7       1.59       18.0  5.92e-17\n2 hp           -0.0240    0.0154     -1.56 1.30e- 1\n3 cyl6         -5.97      1.64       -3.64 1.09e- 3\n4 cyl8         -8.52      2.33       -3.66 1.03e- 3\n```\n:::\n\n```{.r .cell-code}\npdata <- tibble(hp=seq_range(mt$hp,n=200)) %>%\n  expand_grid(cyl=mt$cyl)\npdata$yhat <- predict(model,newdata=pdata)\nggplot() +\n  geom_point(data=mt, aes(x=hp,y=mpg,color=cyl)) +\n  geom_line(data=pdata, aes(x=hp,y=yhat,color=cyl))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n\n### Linear Regression (multiple vars, with interaction)\n\n\n::: {.cell .preview-image}\n\n```{.r .cell-code}\nmodel <- lm(mpg ~ hp + cyl + hp*cyl, data=mt)\ntidy(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 5\n  term        estimate std.error statistic       p.value\n  <chr>          <dbl>     <dbl>     <dbl>         <dbl>\n1 (Intercept)  36.0       3.89        9.25 0.00000000104\n2 hp           -0.113     0.0457     -2.47 0.0206       \n3 cyl6        -15.3       7.43       -2.06 0.0496       \n4 cyl8        -17.9       5.26       -3.40 0.00216      \n5 hp:cyl6       0.105     0.0685      1.54 0.137        \n6 hp:cyl8       0.0985    0.0486      2.03 0.0531       \n```\n:::\n\n```{.r .cell-code}\npdata <- tibble(hp=seq_range(mt$hp,n=200)) %>%\n  expand_grid(cyl=mt$cyl)\npdata$yhat <- predict(model,newdata=pdata)\nggplot() +\n  geom_point(data=mt, aes(x=hp,y=mpg,color=cyl)) +\n  geom_line(data=pdata, aes(x=hp,y=yhat,color=cyl))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Nonlinear Regression\n\nPackage `minpack.lm` uses Levenberg-Marquardt algorithm, which seems to be more forgiving compared with `nls`.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(minpack.lm)\nmodel <- nlsLM(conc ~ a * exp(-b*time), start=list(a=1,b=1), data=Indometh)\ntidy(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 5\n  term  estimate std.error statistic  p.value\n  <chr>    <dbl>     <dbl>     <dbl>    <dbl>\n1 a         2.78    0.154       18.1 8.24e-27\n2 b         1.35    0.0994      13.6 1.58e-20\n```\n:::\n\n```{.r .cell-code}\npdata <- tibble(time=seq_range(Indometh$time,n=200))\npdata$yhat <- predict(model,newdata=pdata)\nggplot() +\n  geom_point(data=Indometh, aes(x=time,y=conc)) +\n  geom_line(data=pdata, aes(x=time,y=yhat),color=\"red\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\n## Logistic Regression\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmt <- mtcars %>% mutate(gas.guzzler=as.numeric(mpg<22.5))\nmodel <- glm(gas.guzzler ~ disp + am, data=mt,family=\"binomial\")\ntidy(model, exponentiate = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 5\n  term          estimate std.error statistic p.value\n  <chr>            <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept) 0.00000705    6.01      -1.97   0.0483\n2 disp        1.09          0.0405     2.07   0.0389\n3 am          4.56          1.83       0.830  0.406 \n```\n:::\n\n```{.r .cell-code}\npdata <- crossing(disp=seq_range(mt$disp,n=200),\n                  am=mt$am)\npdata$yhat <- predict(model,newdata=pdata,type=\"response\")\nggplot() + \n  geom_point(data=mt,aes(x=disp,y=gas.guzzler,color=factor(am))) +\n  geom_line(data=pdata, aes(x=disp,y=yhat,color=factor(am)))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\n## Penalized Regression\n\n### Ridge Regression\n\nUse ridge regression (a.k.a. L2 regularization) to fit a model in order to deal with multicollinearity and overfitting. \nA penalty factor $\\lambda$ is used to minimize high coefficients. \n$\\lambda$ = 0 is equivalent to ordinary least squares, and higher values means higher penalty.\nUsing the `glmnet` package, set `alpha = 0` for ridge regression.\n\nDetermine best $\\lambda$ (produces lowest mean squared error) using k-fold cross-validation, `cv.glmnet()`.\nNote that predictors are standardized by default.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(glmnet)\ny <- mtcars$hp\nx <- mtcars %>% dplyr::select(mpg, wt, drat, qsec) %>% as.matrix()\n\n#find optimal lambda value that minimizes test MSE\ncv_model <- cv.glmnet(x, y, alpha = 0)\nbest_lambda <- cv_model$lambda.min\n\nmodel <- glmnet(x, y, alpha = 0, lambda = best_lambda)\ncoef(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n5 x 1 sparse Matrix of class \"dgCMatrix\"\n                    s0\n(Intercept) 470.705023\nmpg          -3.280904\nwt           18.505501\ndrat         -2.468512\nqsec        -17.298734\n```\n:::\n\n```{.r .cell-code}\n#calculate R-squared\ny_predicted <- predict(model, s = best_lambda, newx = x)\nsst <- sum((y - mean(y))^2)\nsse <- sum((y_predicted - y)^2)\n#find R-Squared\nrsq <- 1 - sse/sst\nrsq\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.7959184\n```\n:::\n:::\n\n\n\n### Lasso Regression\n\nLasso regression is similar but penalty is based on sum of absolute values coefficients.\nSetting `alpha = 1` will perform Lasso (Elastic net is intermediate values for alpha).\nNote that Lasso can shrink coefficients down to zero.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny <- mtcars$hp\nx <- mtcars %>% select(mpg, wt, drat, qsec) %>% as.matrix()\nmodel <- glmnet(x, y, alpha = 1)\n\n#find optimal lambda value that minimizes test MSE\ncv_model <- cv.glmnet(x, y, alpha = 1)\nbest_lambda <- cv_model$lambda.min\n\nmodel <- glmnet(x, y, alpha = 1, lambda = best_lambda)\ncoef(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n5 x 1 sparse Matrix of class \"dgCMatrix\"\n                   s0\n(Intercept) 476.29995\nmpg          -3.14287\nwt           18.68341\ndrat          .      \nqsec        -18.29705\n```\n:::\n\n```{.r .cell-code}\n#calculate R-squared\ny_predicted <- predict(model, s = best_lambda, newx = x)\nsst <- sum((y - mean(y))^2)\nsse <- sum((y_predicted - y)^2)\n#find R-Squared\nrsq <- 1 - sse/sst\nrsq\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.7982125\n```\n:::\n:::\n\n\n\n\n## Mixed Effects \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lme4)\nlibrary(nlme)\n\nview.predict <- function(fit) {\n  data1 %>% mutate(pred_dist = fitted(fit)) %>%\n    ggplot(aes(x=age, y=pred_dist, group=Subject, color=Subject)) + theme_classic() +\n    geom_line(size=1)\n}\n\ndata1 <- Orthodont %>% mutate(Subject=factor(Subject,ordered=FALSE))\ndata1 %>%\n    ggplot(aes(x=age, y=distance, group=Subject, color=Subject, linetype=Sex)) +\n    geom_line(size=1) + theme_classic()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# null model\nmod1 <- lmer(distance ~ (1|Subject), data=data1, REML=F)\nsummary(mod1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by maximum likelihood  ['lmerMod']\nFormula: distance ~ (1 | Subject)\n   Data: data1\n\n     AIC      BIC   logLik deviance df.resid \n   521.5    529.5   -257.7    515.5      105 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.2391 -0.5248 -0.1103  0.4827  2.7734 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n Subject  (Intercept) 3.567    1.889   \n Residual             4.930    2.220   \nNumber of obs: 108, groups:  Subject, 27\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  24.0231     0.4216   56.98\n```\n:::\n\n```{.r .cell-code}\nview.predict(mod1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-2.png){width=672}\n:::\n\n```{.r .cell-code}\n# Random intercept model\nmod2 <- lmer(distance ~ age + (1|Subject), data=data1, REML=F)\nsummary(mod2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by maximum likelihood  ['lmerMod']\nFormula: distance ~ age + (1 | Subject)\n   Data: data1\n\n     AIC      BIC   logLik deviance df.resid \n   451.4    462.1   -221.7    443.4      104 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.6870 -0.5386 -0.0123  0.4910  3.7470 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n Subject  (Intercept) 4.294    2.072   \n Residual             2.024    1.423   \nNumber of obs: 108, groups:  Subject, 27\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) 16.76111    0.79456   21.09\nage          0.66019    0.06122   10.78\n\nCorrelation of Fixed Effects:\n    (Intr)\nage -0.848\n```\n:::\n\n```{.r .cell-code}\nview.predict(mod2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-3.png){width=672}\n:::\n\n```{.r .cell-code}\n# Random intercept and random slope (independent)\nmod3 <- lmer(distance ~ age + (1|Subject) + (0+age|Subject), data=data1, REML=F)\nsummary(mod3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by maximum likelihood  ['lmerMod']\nFormula: distance ~ age + (1 | Subject) + (0 + age | Subject)\n   Data: data1\n\n     AIC      BIC   logLik deviance df.resid \n   449.7    463.1   -219.9    439.7      103 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.7542 -0.5056  0.0181  0.5216  3.8017 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n Subject   (Intercept) 1.82570  1.3512  \n Subject.1 age         0.02141  0.1463  \n Residual              1.85944  1.3636  \nNumber of obs: 108, groups:  Subject, 27\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) 16.76111    0.70816   23.67\nage          0.66019    0.06509   10.14\n\nCorrelation of Fixed Effects:\n    (Intr)\nage -0.822\n```\n:::\n\n```{.r .cell-code}\nview.predict(mod3)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-4.png){width=672}\n:::\n\n```{.r .cell-code}\n# Random intercept and random slope (correlated)\nmod4 <- lmer(distance ~ age + (1+age|Subject), data=data1, REML=F)\nsummary(mod4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by maximum likelihood  ['lmerMod']\nFormula: distance ~ age + (1 + age | Subject)\n   Data: data1\n\n     AIC      BIC   logLik deviance df.resid \n   451.2    467.3   -219.6    439.2      102 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.3060 -0.4874  0.0076  0.4822  3.9228 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n Subject  (Intercept) 4.81397  2.1941        \n          age         0.04619  0.2149   -0.58\n Residual             1.71623  1.3100        \nNumber of obs: 108, groups:  Subject, 27\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) 16.76111    0.76076  22.032\nage          0.66019    0.06992   9.442\n\nCorrelation of Fixed Effects:\n    (Intr)\nage -0.848\n```\n:::\n\n```{.r .cell-code}\nview.predict(mod4)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-5.png){width=672}\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}